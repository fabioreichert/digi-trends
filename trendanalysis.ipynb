{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Analysis\n",
    "\n",
    "This notebook is used to setup all the methods and tools for the analyis of trends in the literature. First, methods that useful in storing, importing and manipulating data are introducted, followed by the class \"TrendAnalysis()\", which serves as the main object to manage searches in Scopus and trend evaluation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "#The above import is only for console\n",
    "import pandas as pd\n",
    "from pybliometrics.scopus import ScopusSearch\n",
    "from pybliometrics.scopus import AbstractRetrieval\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, name, time = None, folder = \"data\"):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame \"df\" under the filename \"name\" with the current time stamp into the default folder \"data\".\n",
    "    \"\"\"\n",
    "    if time is None:\n",
    "        time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    if name.endswith('.csv'):\n",
    "        filename = f'{time} {name}'\n",
    "    else:\n",
    "        filename = f'{time} {name}.csv'\n",
    "        \n",
    "    filepath = os.path.join(os.path.abspath(os.getcwd()), folder, filename)\n",
    "    df.to_csv(filepath)\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename, folder = \"data\"):\n",
    "    \"\"\"\n",
    "    Reads and returns the DataFrame with the name \"filename\" from the default folder \"data\".\n",
    "    \"\"\"\n",
    "    converters = {\n",
    "        \"authkeywords\": lambda x: set(x.strip(\"{}\").replace(\"'\",\"\").split(\", \")),\n",
    "        #\"index_terms\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "    }\n",
    "\n",
    "    # Read data from files\n",
    "    filepath = os.path.join(os.path.abspath(os.getcwd()), folder, filename)\n",
    "    df = pd.read_csv(filepath, index_col = 'eid', converters=converters)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dict_, name, time = None, folder =\"data\"):\n",
    "    \"\"\"\n",
    "    Saves the dictionary \"dict_\" under the name \"name\" with the current time stamp into the default folder \"data\".\n",
    "    \"\"\"\n",
    "    if time is None:\n",
    "        time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "        \n",
    "    filename = f'{time} {name}'\n",
    "    filepath = os.path.join(os.path.abspath(os.getcwd()), folder, filename)\n",
    "    \n",
    "    file = open(filepath, \"w\")\n",
    "    json.dump(dict_, file)\n",
    "    file.close()\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(filename, folder =\"data\"):\n",
    "    \"\"\"\n",
    "    Reads and returns the dictionary with the name \"filename\" from the default folder \"data\".\n",
    "    \"\"\"\n",
    "        \n",
    "    # Read data from files\n",
    "    filepath = os.path.join(os.path.abspath(os.getcwd()), folder, filename)\n",
    "    \n",
    "    file = open(\"data.json\", \"r\")\n",
    "    dict_ = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyword_file(folder, confirm_manually, min_appearances):\n",
    "    \"\"\"\n",
    "    Creates a text file for all new keywords to be stored that are included in the extended search.\n",
    "    \"\"\"\n",
    "    \n",
    "    time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    \n",
    "    filename = f'{time} additional keywords.txt'\n",
    "    \n",
    "    filepath = os.path.join(os.path.abspath(os.getcwd()), folder, filename)\n",
    "    \n",
    "    f = open(filepath, \"x\")\n",
    "    \n",
    "    if confirm_manually:\n",
    "        f.write(\"The following keywords were used in the extended search. Keywords denied by the user can be found in a separate file. \"+\n",
    "                f\"All keywords appeared at least {min_appearances} times in a previous search.\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"The following keywords were used in the extended search. Keywords were not manually confirmed by the user and \"+\n",
    "                f\"automatically included when they appeared more than {min_appearances} times in a previous search.\\n\\n\")\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denied_keyword_file(folder):\n",
    "    \"\"\"\n",
    "    Creates a text file for all keywords that are manually denied by the user.\n",
    "    \"\"\"\n",
    "    time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "\n",
    "    filename = f'{time} denied keywords.txt'\n",
    "    \n",
    "    filepath = os.path.join(os.path.abspath(os.getcwd()), folder, filename)\n",
    "    \n",
    "    f = open(filepath, \"x\")\n",
    "    f.write(\"The following keywords were manually selected by the user to not be included in the extended search:\\n\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_keyword_file(filepath, content, recursion):\n",
    "    \"\"\"\n",
    "    Opens the file specified by the filepath and inserts the content and the recursion in which it was called.\n",
    "    \"\"\"\n",
    "    f = open(filepath, \"a\")\n",
    "    \n",
    "    if(isinstance(content, str)):\n",
    "        f.write(f'{recursion}: {content}\\n')\n",
    "    elif(isinstance(content, list)):\n",
    "        for keyword in content:\n",
    "            f.write(f'{recursion}: {keyword}\\n')\n",
    "    else:\n",
    "        print(f\"When trying to write to {filepath}, the input was neither a string nor a list. Trying string conversion. \" + \n",
    "             \"It is recommended to check for validity in the file.\")\n",
    "        f.write(f'{recursion}: {str(content)}\\n')\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword and data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keywords(series):\n",
    "    \"\"\"\n",
    "    Converts the keyword data retrieved from Scopus in a form so that it can be more easily used in the program.\n",
    "    \n",
    "    Example of doc[i][\"authkeywords\"] to visualize the nested characteristic of the data set:\n",
    "        \n",
    "        {'author-keyword': [{'@_fa': 'true', '$': 'Information'},\n",
    "              {'@_fa': 'true', '$': 'Network'},\n",
    "              {'@_fa': 'true', '$': 'Optimization'},\n",
    "              {'@_fa': 'true', '$': 'Production'},\n",
    "              {'@_fa': 'true', '$': 'Simulation'}]}\n",
    "    \"\"\"\n",
    "    \n",
    "    author_keywords = [None] * len(series)\n",
    "    doc_keywords = []\n",
    "    not_found = 0\n",
    "    \n",
    "    for i in range(0, len(author_keywords)):\n",
    "        entry = series[i]\n",
    "        if entry != None:\n",
    "            if '|' in entry:\n",
    "                keywords = entry.lower().split(' | ')\n",
    "                keywords = {transform(keyword) for keyword in keywords}\n",
    "            else:\n",
    "                keywords = entry.lower().split()\n",
    "                keywords = {transform(keyword) for keyword in keywords}\n",
    "            author_keywords[i] = keywords\n",
    "    \n",
    "    return pd.Series(author_keywords, index=series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(string):\n",
    "    \"\"\"\n",
    "    Plurals are removed for common words to avoid the cases where keywords or index terms are retrieved in plural and singular forms.\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(string)\n",
    "    \n",
    "    dict_ = {\n",
    "        # british american spelling\n",
    "        \"digitalisation\": \"digitalization\",\n",
    "        \"digitisation\": \"digitization\",\n",
    "        \"digitised\": \"digitized\",\n",
    "        \"optimisation\": \"optimization\",\n",
    "        \"servitisation\": \"servitization\",\n",
    "        \n",
    "        # plural removal\n",
    "        \"assessments\": \"assessment\",\n",
    "        \"automations\": \"automation\",\n",
    "        \"chains\": \"chain\",\n",
    "        \"changes\": \"change\",\n",
    "        \"companies\": \"company\",\n",
    "        \"contracts\": \"contract\",\n",
    "        \"decisions\": \"decision\",\n",
    "        \"dependencies\": \"dependency\",\n",
    "        \"experiments\": \"experiment\",\n",
    "        \"impacts\": \"impact\",\n",
    "        \"industries\": \"industry\",\n",
    "        \"informations\": \"information\",\n",
    "        \"interviews\": \"interview\",\n",
    "        \"managements\": \"management\",\n",
    "        \"models\": \"model\",\n",
    "        \"networks\": \"network\",\n",
    "        \"processes\": \"process\",\n",
    "        \"projects\": \"project\",\n",
    "        \"relationships\": \"relationship\",\n",
    "        \"researches\": \"research\",\n",
    "        \"revolutions\": \"revolution\",\n",
    "        \"services\": \"service\",\n",
    "        \"systems\": \"system\",\n",
    "        \"technologies\": \"technology\",\n",
    "        \"transfers\": \"transfer\",\n",
    "        \"transformations\": \"transformation\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "    normalized_tokens = []\n",
    "    \n",
    "    for word in tokenized:\n",
    "        if word in dict_:\n",
    "            normalized_tokens.append(dict_[word])\n",
    "        else:\n",
    "            normalized_tokens.append(word)\n",
    "\n",
    "    return ' '.join(normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_remove_duplicates(dfs):\n",
    "    \"\"\"\n",
    "    Returns a concatenated dataframe without duplicate entries.\n",
    "    \"\"\"\n",
    "    appended_dfs = pd.concat(dfs)\n",
    "    original_length = len(appended_dfs)\n",
    "    appended_dfs = appended_dfs[~appended_dfs.index.duplicated(keep='first')]\n",
    "    new_length = len(appended_dfs)\n",
    "    \n",
    "    n_duplicates = original_length - new_length\n",
    "    if(n_duplicates == 0):\n",
    "        print(\"None of the search results were duplicates.\\n\")\n",
    "    elif(n_duplicates == 1 ):\n",
    "        print(f\"{n_duplicates} result was a duplicate.\\n\")\n",
    "    else:\n",
    "        print(f\"{n_duplicates} of the results were duplicates.\\n\")\n",
    "        \n",
    "    return appended_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entries(series):\n",
    "    \"\"\"\n",
    "    Returns a Counter object for all keywords that appear in the series.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "\n",
    "    for list_ in series:\n",
    "        if list_ is not None:\n",
    "            try:\n",
    "                for entry in list_:\n",
    "                    entries.append(entry)\n",
    "            except:\n",
    "                print(list_)\n",
    "    return Counter(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_publications(df, column_name = None):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of time series of total publications and publications without keywords.\n",
    "    \"\"\"\n",
    "    # Create date index\n",
    "    cover_dates = pd.to_datetime(df[\"coverDate\"])\n",
    "    min_date = cover_dates.min()\n",
    "    max_date = cover_dates.max()\n",
    "    index_col = pd.date_range(min_date, max_date, freq=\"d\")\n",
    "    \n",
    "    # Create DataFrame \n",
    "    publications = pd.DataFrame(0, index=index_col, columns=['publications', 'no_keywords'])\n",
    "\n",
    "    # Fill data frame    \n",
    "    for doc in df.index:\n",
    "        date = cover_dates[doc]\n",
    "        publications['publications'][date] += 1\n",
    "        if df['authkeywords'][doc] is None:\n",
    "            publications['no_keywords'][date] += 1\n",
    "    return publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendAnalysis():\n",
    "    \"\"\"\n",
    "    This class is used to retrieve, store and analyze literature data from Scopus. It is based on two sets of search terms that are used to identify relevant publications in Scopus.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_terms, trend_terms, to_file = True, refresh = True, folder = 'data', max_year = 2020, min_year = 2000, gf_base_year = 2015):\n",
    "        \"\"\"\n",
    "        Initialized the TrendAnalysis object. This object is used for all search and primary analysis purposes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        to_file : Specifies whether interim and final results shall be saved in a local file in the project folder.\n",
    "        \n",
    "        refresh : Controls whether all searches in Scopus shall be rerun or loaded from cashed files on the system.\n",
    "\n",
    "        folder : Folder in the project folder where the results shall be saved.\n",
    "        \n",
    "        max_year : Searches in Scopus will only regard documents that were published until this year. max_year will also be used to calculate the percentage change of publications for a certain topic until this year. Default 2020.\n",
    "        \n",
    "        min_year : Searches in Scopus will only regard documents that were published from this year onward. Default 2000.\n",
    "        \n",
    "        gf_base_year : To calculate the percentage change of publications for a certain topic (GF) this year is used as the base yea\n",
    "        \"\"\"  \n",
    "        # Check if the input has the right format. If only one argument is given in the form of a string it will be wrapped in a list.\n",
    "        if type(base_terms) != list:\n",
    "            if type(base_terms) is str:\n",
    "                self.st1 = [base_terms]\n",
    "            else:\n",
    "                print(f\"The given input argument for production keywords should be a list but is {type(base_terms)}\")\n",
    "                return None\n",
    "        else:\n",
    "            self.st1 = base_terms\n",
    "        if type(trend_terms) != list:\n",
    "            if type(trend_terms) is str:\n",
    "                self.st2 = [trend_terms]\n",
    "            else:\n",
    "                print(f\"The given input argument for digitalisation keywords should be a list but is {type(trend_terms)}\")\n",
    "                return None\n",
    "        else:    \n",
    "            self.st2 = trend_terms\n",
    "        \n",
    "        self.to_file = to_file\n",
    "        self.refresh = refresh\n",
    "        self.folder = folder\n",
    "        self.min_year = min_year\n",
    "        self.max_year = max_year\n",
    "        self.gf_base_year = gf_base_year\n",
    "        self.st_ts = None\n",
    "        \n",
    "    def reset_covered_kws(self):\n",
    "        \"\"\"\n",
    "        Reset the covered keywords to those of the initial search.\n",
    "        \"\"\"\n",
    "        self.covered_kws = self.st1 + self.st2 + ['', \"manufacturing\", \"production\", \"manufacturing network\", \"digital transformation\"]\n",
    "            \n",
    "        return self.covered_kws\n",
    "    \n",
    "    def remove_duplicates(self, dict_):\n",
    "        \"\"\"\n",
    "        Ensures that for every key in the dictionary, the associated entry does not contain any duplicates. Returns this dictionary.\n",
    "        \"\"\"\n",
    "        cleaned = dict_\n",
    "        for key in cleaned:\n",
    "            cleaned[key] = list(set(cleaned[key]))\n",
    "        return cleaned\n",
    "        \n",
    "        \n",
    "    def run_initial_search(self):\n",
    "        \"\"\"\n",
    "        Runs an initial search for the explicitly specified search term lists.\n",
    "    \n",
    "        \"\"\"\n",
    "        # Search for all queries\n",
    "        print(\"Commencing search...\")\n",
    "        appended_dfs = []\n",
    "        srch_affil = {search_term: [] for search_term in (self.st1+self.st2)}\n",
    "        for term1 in self.st1:\n",
    "            for term2 in self.st2:\n",
    "                query = f'TITLE-ABS-KEY(\"{term1}\" AND \"{term2}\") AND PUBYEAR > {self.min_year-1} AND PUBYEAR < {self.max_year+1}'\n",
    "                search = ScopusSearch(query, verbose=True, refresh=self.refresh)\n",
    "                results = pd.DataFrame(search.results)\n",
    "                print(f'{len(results)} results downloaded for query \"{query}\".')\n",
    "                if not results.empty:\n",
    "                    results = results.set_index('eid')\n",
    "                    appended_dfs.append(results)\n",
    "                    srch_affil[term1] += list(results.index)\n",
    "                    srch_affil[term2] += list(results.index)\n",
    "           \n",
    "\n",
    "        # Concatenate and drop duplicates\n",
    "        appended_dfs = concat_and_remove_duplicates(appended_dfs)\n",
    "\n",
    "    \n",
    "        appended_dfs['authkeywords'] = convert_keywords(appended_dfs['authkeywords'])\n",
    "        appended_dfs['citedby_count'] = appended_dfs['citedby_count'].astype(int)\n",
    "\n",
    "\n",
    "        self.initial_results = appended_dfs\n",
    "        self.initial_dict = self.remove_duplicates(srch_affil)\n",
    "        self.reset_covered_kws()\n",
    "        self.ts = None\n",
    "\n",
    "        # Write to disk        \n",
    "        if self.to_file:\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "            save_df(appended_dfs, \"initial results\", time=time)\n",
    "            save_dict(srch_affil, \"dictionary\", time=time)\n",
    "        \n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def run_extended_search(self, initial_min = 10, iterations = 1, min_appearances = 100):\n",
    "        \"\"\"\n",
    "        Runs an extended search for the most frequently occurring keywords in the initial results and previous iterations.\n",
    "        For each keyword, a dictionary entry is stored with all corresponding document ids of the search.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        iterations : 1 by default. Change to a value larger than 1 to allow for further iterations.\n",
    "        \n",
    "        initial_min : 10 by default. Minimum number of occurrences of an author keyword to be proposed for the first iteration.\n",
    "        \n",
    "        min_appearances : 100 by default. Minimum appearances for the second iteration. This number will double after each iteration.\n",
    "                          Only has an effect, if the number of iterations is larger than 1.\n",
    "        \"\"\"\n",
    "        # Set default minimum appearances.\n",
    "        \n",
    "        if iterations == 1:\n",
    "             print(f\"\\nThe search will run for {iterations} iteration and select all keywords that occur more than {initial_min} in the initial results.\")\n",
    "    \n",
    "        elif (iterations > 1) & (type(iterations) == int):\n",
    "            print(f\"\\nThe search will select all keywords that occur more than {initial_min} in the initial results for the first iteration. In the second iteration,\"+\n",
    "                 f\" the search will start with {min_appearances} minimum keyword occurrences and double for each of the remaining {iterations-1} iterations.\")\n",
    "        else:\n",
    "            print(\"The parameter 'iterations' should be set to an integer number larger or equal to 1. Please try again.\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        # Create the list of additional keywords by looking at the most common occurences and filtering out certain topics.\n",
    "        counter = count_entries(self.initial_results['authkeywords'])\n",
    "        most_common = counter.most_common()\n",
    "\n",
    "        # Manually blacklisted keywords that have been covered in the initial search.\n",
    "        covered_keywords = self.covered_kws\n",
    "\n",
    "        additional_keywords = []\n",
    "        covered = covered_keywords.copy()\n",
    "        covered.remove('')\n",
    "        for entry in most_common:\n",
    "            if entry[0] not in covered_keywords:\n",
    "                if entry[1] >= initial_min:\n",
    "                    # Check that the new keyword is not a part of any previously covered keyword\n",
    "                    if not any((keyword in entry[0]) for keyword in covered):\n",
    "                        additional_keywords.append(entry[0])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # Stop here if no additional keywords are found.\n",
    "        if len(additional_keywords) == 0:\n",
    "            print(\"No additional keywords found that meet criteria. Consider lowering the minimum number of appearances required.\")\n",
    "            print(\"(If the extended search has been run before, you might can use self.reset_covered_kws() to reset the covered keywords to those of the initial search.)\")\n",
    "            return None\n",
    "\n",
    "        # Let user decide whether he/she wants to confirm additional keywords\n",
    "        print(f\"\\nDo you want to confirm these additional keywords manually at each iteration ? (Y/N).\")\n",
    "        confirm_manually = True\n",
    "        while True:\n",
    "            user_input = input()\n",
    "            if user_input == 'y' or user_input == 'Y':\n",
    "                if self.to_file:\n",
    "                    denied_keyword_file = create_denied_keyword_file(self.folder)\n",
    "                break\n",
    "            elif user_input == 'n' or user_input == 'N':\n",
    "                confirm_manually = False\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a valid input like 'Y' or 'N'.\")\n",
    "\n",
    "        if self.to_file:\n",
    "            keyword_file = create_keyword_file(self.folder, confirm_manually, min_appearances)\n",
    "\n",
    "        # Iterating through keywords\n",
    "        covered_documents = self.initial_results.index\n",
    "        recursion = 0\n",
    "        failed_to_read = []\n",
    "        srch_affil = {search_term: [] for search_term in (self.st1+additional_keywords)}\n",
    "        extended_results = self.initial_results \n",
    "        \n",
    "        while (len(additional_keywords) != 0) and recursion < iterations:\n",
    "            \n",
    "            # Print most frequently occurring keywords if user chose to confirm manually.\n",
    "            if confirm_manually:\n",
    "                if recursion == 0:\n",
    "                    print(f\"\\nThe most common keywords that appear more than {initial_min} times are:\")\n",
    "                else:\n",
    "                    print(f\"\\nThe most common keywords that appear more than {min_appearances} times are:\")\n",
    "                for keyword in additional_keywords:\n",
    "                    print(f\"\\t{keyword} ({counter[keyword]})\")\n",
    "                print(\"To remove one of the above keywords from the search enter it below (e.g. 'big data'). \"+\n",
    "                 \"Otherwise enter 'N'. If you want to stop altogether enter 'STOP'.\")\n",
    "    \n",
    "            # Let user confirm additional keywords at each iteration.\n",
    "            min_appearances = min_appearances * pow(2, recursion)\n",
    "            while True:\n",
    "                user_input = input()\n",
    "                if user_input in additional_keywords:\n",
    "                    additional_keywords.remove(user_input)\n",
    "                    del srch_affil[user_input]\n",
    "                    covered_keywords.append(user_input)\n",
    "                    # Write denied keyword to log file.\n",
    "                    if self.to_file:\n",
    "                        write_to_keyword_file(denied_keyword_file, user_input, recursion)\n",
    "                    print(f\"\\nRemoved '{user_input}' from additional keywords. The new list is:\")\n",
    "                    for keyword in additional_keywords:\n",
    "                        print(f\"\\t{keyword} ({counter[keyword]})\")\n",
    "                    print(\"To remove another keyword from the search enter it below (e.g. 'big data'). \"+\n",
    "                         \"Otherwise enter 'N'.\")\n",
    "                elif user_input == 'n' or user_input == 'N':\n",
    "                    break\n",
    "                elif user_input == 'STOP' or user_input == 'stop':\n",
    "                    print(\"This might not return valid results if no iteration has been run.\")\n",
    "                    # 7. Save final results \n",
    "                    self.covered_kws = covered_keywords\n",
    "                    self.ext_results = extended_results\n",
    "                    self.ext_dict = self.remove_duplicates(srch_affil)\n",
    "                    print(\"Search finished.\")\n",
    "                    if self.to_file:\n",
    "                        new_filename = save_df(extended_results, 'extended results') \n",
    "                        print(f\"Extended results are also saved in {new_filename}\")    \n",
    "                    return None\n",
    "                else:\n",
    "                    print(\"Please enter a valid keyword to remove or enter 'N' to accept current keyword selection. Or if you want to stop enther 'STOP'\") \n",
    "\n",
    "            # Create dictionary of search results for each term\n",
    "            dict_ = {search_term: [] for search_term in (self.st1+additional_keywords)}\n",
    "                \n",
    "            # Search for all queries\n",
    "            results = [extended_results]\n",
    "            print(\"\\nCommencing search...\")\n",
    "            for term1 in self.st1:\n",
    "                for term2 in additional_keywords:\n",
    "                    query = f'TITLE-ABS-KEY(\"{term1}\" AND \"{term2}\") AND PUBYEAR > {self.min_year-1} AND PUBYEAR < {self.max_year+1}'\n",
    "                    search = ScopusSearch(query, verbose=True, refresh=self.refresh)\n",
    "\n",
    "                    result = pd.DataFrame(search.results)\n",
    "                    print(f'{len(result)} results downloaded for query \"{query}\"\".')\n",
    "\n",
    "                    if not result.empty:\n",
    "                        result = result.set_index('eid')\n",
    "                        result['authkeywords'] = convert_keywords(result['authkeywords'])\n",
    "                        for term in [term1, term2]:\n",
    "                            if term in srch_affil.keys():\n",
    "                                srch_affil[term] += list(result.index)\n",
    "                            else:\n",
    "                                srch_affil[term] = []\n",
    "                                srch_affil[term] += list(result.index)\n",
    "                        # Drop already covered documents.\n",
    "                        result.drop(covered_documents.intersection(result.index), inplace=True)            \n",
    "                        # Update covered documents and keywords.\n",
    "                        covered_documents = covered_documents.union(result.index)\n",
    "                        # Append to list of data frames\n",
    "                        results.append(result)\n",
    "\n",
    "\n",
    "            # Write keywords to log file\n",
    "            if self.to_file:\n",
    "                write_to_keyword_file(keyword_file, additional_keywords, recursion)\n",
    "\n",
    "            # Update the already covered search terms and recalculate most common keywords and \n",
    "            covered_keywords += additional_keywords\n",
    "            additional_keywords = []\n",
    "            extended_results = concat_and_remove_duplicates(results)\n",
    "            counter = count_entries(extended_results['authkeywords'])\n",
    "            most_common = counter.most_common()\n",
    "\n",
    "            # Create a copy of covered_keywords that does not contain the following keywords and pre-select new terms for the next iteration\n",
    "            covered = covered_keywords.copy()\n",
    "            covered.remove('')\n",
    "            covered.remove('production')\n",
    "            covered.remove('manufacturing')\n",
    "            for entry in most_common:\n",
    "                if entry[0] not in covered_keywords:\n",
    "                    if entry[1] >= min_appearances:\n",
    "                        # Check that the new keyword is not a part of any previously covered keyword\n",
    "                        if not any(keyword in entry[0] for keyword in covered):\n",
    "                            additional_keywords.append(entry[0])\n",
    "                    else:\n",
    "                        break\n",
    "            recursion += 1\n",
    "\n",
    "\n",
    "        # Save final results after all iterations are complete\n",
    "        extended_results['citedby_count'] = extended_results['citedby_count'].astype(int)\n",
    "        if self.to_file:\n",
    "            new_filename = save_df(extended_results, 'extended results')\n",
    "            print(self.to_file)\n",
    "            print(f\"Search finished. Extended results are also saved in {new_filename}\")\n",
    "        self.covered_kws = covered_keywords\n",
    "        self.ext_results = extended_results\n",
    "        self.ext_dict = self.remove_duplicates(srch_affil)\n",
    "        \n",
    "        \n",
    "    def create_kw_ts(self, keyword_column = 'authkeywords', threshold = 10):\n",
    "        \"\"\"\n",
    "        Creates a dataframe of time series for every keyword that appears at least as many times as specified in the\n",
    "        threshold. Every entry represents the number of documents in which a certain author keyword appears in a specific year.\n",
    "\n",
    "        Results are stored in self.ts\n",
    "        \n",
    "        Note: Currently unused.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create date index\n",
    "        print(\"Creating time series of publications per keyword...\")\n",
    "        \n",
    "        cover_dates = pd.to_datetime(self.ext_results[\"coverDate\"])\n",
    "\n",
    "        min_date = cover_dates.min()\n",
    "        max_date = cover_dates.max()\n",
    "\n",
    "        index_col = pd.date_range(min_date, max_date, freq=\"d\")\n",
    "\n",
    "        # Get keywords\n",
    "        keywords = []\n",
    "        counted_keywords = count_entries(self.ext_results[keyword_column])\n",
    "        for tuple_ in counted_keywords.most_common():\n",
    "            keyword = tuple_[0]\n",
    "            appearances = tuple_[1]\n",
    "\n",
    "            if appearances >= threshold:\n",
    "                keywords.append(keyword)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Create empty dataframes for every time series\n",
    "        ts = pd.DataFrame(0, index=index_col, columns=keywords)\n",
    "\n",
    "        # Fill data frame    \n",
    "        for document in tqdm(self.ext_results.index):\n",
    "            try:\n",
    "                for keyword in self.ext_results[keyword_column][document]:\n",
    "                    if keyword in ts.columns:\n",
    "                        cover_date = self.ext_results['coverDate'][document]\n",
    "                        ts[keyword][cover_date] += 1\n",
    "            except TypeError:\n",
    "                pass\n",
    "        \n",
    "        self.ts = ts  \n",
    "    \n",
    "    def create_search_ts(self):\n",
    "        \"\"\"\n",
    "        Creates a dataframe of time series for every search term in the subsequent/extended search. \n",
    "        Each entry represents the number of documents that were identified for the specific search term in a given year.\n",
    "        \n",
    "        Results are stored in self.st_ts\n",
    "\n",
    "        \"\"\"\n",
    "        # Create date index\n",
    "        print(\"Creating time series of publications per keyword...\")\n",
    "\n",
    "        cover_dates = pd.to_datetime(self.ext_results[\"coverDate\"])\n",
    "\n",
    "        min_date = cover_dates.min()\n",
    "        max_date = cover_dates.max()\n",
    "\n",
    "        index_col = pd.date_range(min_date, max_date, freq=\"d\")\n",
    "\n",
    "        # Get keywords\n",
    "        keywords = self.ext_dict.keys()\n",
    "\n",
    "        # Create empty dataframes for every time series\n",
    "        ts = pd.DataFrame(0, index=index_col, columns=keywords)\n",
    "\n",
    "        # Fill data frame    \n",
    "        for keyword in tqdm(keywords):\n",
    "            for document in self.ext_dict[keyword]:\n",
    "                try:\n",
    "                    cover_date = self.ext_results['coverDate'][document]\n",
    "                    ts[keyword][cover_date] += 1\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "        self.st_ts = ts  \n",
    "        \n",
    "    \n",
    "    def plot_analysis(self):\n",
    "        \"\"\"\n",
    "        Plots and prints information on the trends in descending order of their growth factor (GF).\n",
    "        \n",
    "        Printed are the number of documents per year per trend as well as the percentage of total publications, \n",
    "        the most cited documents per trend and the most frequently occurring documents for each trend.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.st_ts is None:\n",
    "            self.total_pub = get_total_publications(self.ext_results).resample('YS').sum()            \n",
    "            self.create_search_ts()\n",
    "            self.st_ts_a = self.st_ts.resample('YS').sum()\n",
    "            self.st_ts_share_a = self.st_ts_a.divide(self.total_pub['publications'], axis = 0)\n",
    "            \n",
    "            \n",
    "        # Rank publications, citations and growth factors.\n",
    "        pub_count = {}\n",
    "        cit_count = {}\n",
    "        gf = {}\n",
    "        \n",
    "        for trend in self.ext_dict.keys():\n",
    "            if trend not in self.st1:\n",
    "                pub_count[trend] = len(self.ext_dict[trend])\n",
    "                cit_count[trend] = self.ext_results.loc[self.ext_dict[trend]].citedby_count.sum()\n",
    "                gf[trend] = float(self.st_ts_share_a[trend][str(self.max_year)].values-self.st_ts_share_a[trend][str(self.gf_base_year)].values)\n",
    "        \n",
    "        r = {key: rank for rank, key in enumerate(sorted(set(pub_count.values()), reverse=True), 1)}\n",
    "        pub_r = {k: r[v] for k,v in pub_count.items()}\n",
    "        \n",
    "        r = {key: rank for rank, key in enumerate(sorted(set(cit_count.values()), reverse=True), 1)}\n",
    "        cit_r = {k: r[v] for k,v in cit_count.items()}\n",
    "\n",
    "        r = {key: rank for rank, key in enumerate(sorted(set(gf.values()), reverse=True), 1)}\n",
    "        gf_r = {k: r[v] for k,v in gf.items()}\n",
    "        \n",
    "        # Save rankings\n",
    "        data = list(zip(gf.values(), pub_count.values(), cit_count.values()))\n",
    "        self.rankings = pd.DataFrame(data = data, index = pub_count.keys(), columns = ['Growth', 'Publications', 'Citations'])\n",
    "\n",
    "    \n",
    "        # Plot graphs by GF\n",
    "        for trend, v in sorted(gf.items(), key=lambda item: item[1], reverse=True):\n",
    "\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=1,\n",
    "                subplot_titles=(f\"Number of documents retrieved for '{trend}' per year.\",f\"Percentage of documents with {trend} in title/abstract/keywords.\"),\n",
    "                print_grid=False)\n",
    "            \n",
    "            # Plot number of publications per year\n",
    "            fig.add_trace(go.Scatter(x=self.st_ts_a['2010':str(self.max_year)].index, y=self.st_ts_a[trend]['2010':str(self.max_year)]), row=1, col=1)\n",
    "            # Plot share of publications per year\n",
    "            fig.add_trace(go.Scatter(x=self.st_ts_share_a['2010':str(self.max_year)].index, y=self.st_ts_share_a[trend]['2010':str(self.max_year)]), row=2, col=1)\n",
    "            fig.update_layout(height=600, width=600, title_text=trend.upper(), template='simple_white', showlegend=False, yaxis2=dict(tickformat=\".2%\", range= [0,0.5]))\n",
    "            \n",
    "            # Print summary statistics\n",
    "            print(trend.upper())\n",
    "            print(f'Total number of publications: {pub_count[trend]} (Rank: {pub_r[trend]})')\n",
    "            print(f'Total number of citations: {cit_count[trend]} (Rank: {cit_r[trend]})')\n",
    "            print(f'Total growth rate of publications as share of total (2015-2020): {round(gf[trend]*100, 2)}% (Rank: {gf_r[trend]})')\n",
    "\n",
    "            fig.show()\n",
    "            \n",
    "            # Print most frequently occurring keywords\n",
    "            print(\"Number of documents with a specific keyword:\")\n",
    "            for keyword in count_entries(self.ext_results.loc[self.ext_dict[trend]].authkeywords).most_common()[:15]:\n",
    "                print(f\"\\t{keyword[0]}: {keyword[1]}\")\n",
    "                \n",
    "\n",
    "            # Print top 5 documents by citations\n",
    "            print(f\"\\n\\nThe most-cited documents in the context of production networks and {trend} are:\")\n",
    "            relevant = self.ext_results.loc[self.ext_dict[trend]]\n",
    "            top_5 = relevant.citedby_count.sort_values(ascending=False)[:5]\n",
    "            for doc in top_5.index:\n",
    "                print(f\"\\tTitle: {relevant.loc[doc].title}\")\n",
    "                print(f\"\\tAuthors: {relevant.loc[doc].author_names}\")\n",
    "                print(f\"\\tCitations: {top_5[doc]}\")\n",
    "                print(f\"\\tKeywords: {relevant.loc[doc].authkeywords}\\n\")\n",
    "\n",
    "    def calc_combined_gf(self, terms, filename = None):\n",
    "        \"\"\"\n",
    "        Calculates the GF when combining synonymous terms. \"terms\" should be a list of two strings.\n",
    "        \n",
    "        If the time series of the combined terms shall be saved, provide a filename in the form of a string.\n",
    "        \"\"\"\n",
    "        if (isinstance(terms, list)) & (len(terms) == 2):\n",
    "            cover_dates = pd.to_datetime(self.ext_results[\"coverDate\"])\n",
    "            min_date = cover_dates.min()\n",
    "            max_date = cover_dates.max()\n",
    "            index_col = pd.date_range(min_date, max_date, freq=\"d\")\n",
    "\n",
    "            ts = pd.DataFrame(0, index=index_col, columns=['trend'])\n",
    "            for document in set(self.ext_dict[terms[0]]+self.ext_dict[terms[1]]):\n",
    "                cover_date = self.ext_results['coverDate'][document]\n",
    "                ts['trend'][cover_date] += 1\n",
    "\n",
    "            ts_a = ts.resample('YS').sum()\n",
    "            ts_a_share = ts_a.divide(self.total_pub['publications'], axis = 0)\n",
    "            if isinstance(filename, str):\n",
    "                ts_a.to_csv(filename)\n",
    "            gf = float(ts_a_share.loc[str(self.max_year)].values-ts_a_share.loc[str(self.gf_base_year)].values)\n",
    "            print(f\"The GF for {terms[0]} and {terms[1]} is: {round(gf*100,2)}%\")\n",
    "        else:\n",
    "            print(\"Please enter a list of two strings, e.g. '[\\\"industry 4.0\\\", \\\"industrie 4.0\\\"]'\")\n",
    "            return           \n",
    "      \n",
    "    \n",
    "    def export_top_pub_table(self, search_terms = None, number = 10):\n",
    "        \"\"\"\n",
    "        Exports data on the top publications. If no argument is given, the data on the most frequently cited documents \n",
    "        from the extended search will be exported. If an export for specific topics is desired, enter a single topic as a \n",
    "        string or multiple ones as list.\n",
    "        \"\"\"\n",
    "        # Check for search_terms type and initialize filename and choose the relevant documents\n",
    "        if isinstance(search_terms, str):\n",
    "            top = self.ext_results.loc[self.ext_dict[search_terms]].citedby_count.sort_values(ascending=False)[:number].index\n",
    "            filename = f\"{search_terms} top {number}.csv\"\n",
    "        elif isinstance(search_terms, list):\n",
    "            docs = []\n",
    "            for st in search_terms:\n",
    "                docs += self.ext_dict[st]\n",
    "            top = self.ext_results.loc[set(docs)].citedby_count.sort_values(ascending=False)[:number].index\n",
    "            filename = f\"{search_terms[0]} top {number}.csv\"\n",
    "        elif search_terms is None:\n",
    "            top = self.ext_results.citedby_count.sort_values(ascending=False)[:number].index\n",
    "            filename = f\"Overall top {number}.csv\"\n",
    "        else:\n",
    "            print(\"Error: Enter a search term as string or a list of search_terms\")\n",
    "            return\n",
    "\n",
    "        subtypes =  []\n",
    "        authors = []\n",
    "        citations = []\n",
    "        titles = []\n",
    "        separator = ', '\n",
    "        \n",
    "        # Retrieve detailed data on these publications\n",
    "        for doc in top:\n",
    "            ab = AbstractRetrieval(doc)\n",
    "\n",
    "            subtypes.append(ab.subtypedescription)\n",
    "            if len(ab.authors) >= 2:\n",
    "                authors.append(f'{ab.authors[0].surname} et al. ({ab.coverDate.split(\"-\")[0]})')\n",
    "            else:\n",
    "                authors.append(f'{ab.authors[0].surname} ({ab.coverDate.split(\"-\")[0]})')\n",
    "\n",
    "            citations.append(self.ext_results.loc[doc].citedby_count)    \n",
    "            titles.append(ab.title)\n",
    "\n",
    "        table = pd.DataFrame(columns = [\"Category\", \"Author (Year)\", \"Titles\", \"Citations\"])\n",
    "        table[\"Category\"] = subtypes\n",
    "        table[\"Author (Year)\"] = authors\n",
    "        table[\"Citations\"] = citations\n",
    "        table[\"Titles\"] = titles\n",
    "        table = table.set_index(\"Category\")\n",
    "        table = table.sort_values(by =['Category', 'Citations'], ascending = [True, False])\n",
    "        table.to_csv(filename)\n",
    "        print(f\"Table exported into project folder as '{filename}'.\")\n",
    "        \n",
    "        \n",
    "    def export_for_vos(self, search_terms = None):\n",
    "        \"\"\"\n",
    "        Exports author keywords so they can be displayed in a thematic map in VOSviewer. If no argument \n",
    "        is given, the author keywords for all documents from the extended search will be exported.\n",
    "        If an export for specific topics is desired, enter a single topic as a string or multiple ones as list.\n",
    "        \"\"\"\n",
    "        # Check for the type of search_terms and initialize the filename and the author keywords of the relevant documents\n",
    "        kws = []\n",
    "        if isinstance(search_terms, str):\n",
    "            doc_ids = self.ext_dict[search_terms]\n",
    "            base = self.ext_results.loc[doc_ids].authkeywords\n",
    "            filename = f'vos_{search_terms}.csv'\n",
    "        elif isinstance(search_terms, list):\n",
    "            doc_ids = []\n",
    "            topic = \"\"\n",
    "            for term in search_terms:\n",
    "                doc_ids += self.ext_dict[term]\n",
    "                topic += \"_\"+ term\n",
    "            doc_ids = set(doc_ids)\n",
    "            base = self.ext_results.loc[doc_ids].authkeywords\n",
    "            filename = f'vos{topic}.csv'\n",
    "        elif search_terms is None:\n",
    "            base = self.ext_results.authkeywords\n",
    "            filename = f'vos_full_export.csv'\n",
    "        else:\n",
    "            print(\"Please enter a string or list of search terms to export for specific topics or call without an argument for a full export.\")\n",
    "            return\n",
    "        \n",
    "        # Store the author keywords in the form required by VOS.\n",
    "        for doc in base:\n",
    "            if doc is None:\n",
    "                kws.append(\"\")\n",
    "            else:\n",
    "                kws.append(\"; \".join(list(doc)))\n",
    "        # Export keywords to csv in project folder\n",
    "        pd.DataFrame(index = base.index, data = kws, columns=['Author Keywords']).to_csv(filename)\n",
    "        print(f\"Data exported for VOS into project folder as '{filename}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
